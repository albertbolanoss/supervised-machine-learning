{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3afe5ce4",
   "metadata": {},
   "source": [
    "# Regresion model tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c5d7e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best Ridge regression model (L2 Norm)...\n",
      "Best Model Found:\n",
      "Degree: 2\n",
      "Alpha: 0.0010\n",
      "Train MAE: 40.93\n",
      "Validation MAE: 37.74\n",
      "Test MAE: 44.47\n",
      "Coefficients: [ 7.43325137e+01 -2.51582579e+02  5.45836600e+02  3.59550755e+02\n",
      " -7.95721260e+02  5.89908704e+02 -4.71208576e+01  2.16861788e+01\n",
      "  6.60899536e+02  3.69726964e+01  1.13409726e+03  1.74778684e+03\n",
      " -4.22837370e+02  9.50042527e+02 -2.18753685e+02 -6.87804634e+02\n",
      "  3.11279913e+01  5.01180086e+02  1.05996678e+03  6.82979947e+02\n",
      " -1.51917693e+00  4.00072587e+02  7.30949446e+02  4.90842257e+02\n",
      "  5.00878322e+01  7.28529790e+02 -6.01115466e+02  4.35892073e+02\n",
      "  1.19617989e+02  6.42904593e+02  1.70794842e+03 -5.10952040e+02\n",
      " -8.14088704e+01 -6.69023830e+02  4.42486468e+02  1.95327635e+02\n",
      "  8.92354724e+02 -2.38424931e+02  4.01347881e+02  1.33227593e+02\n",
      "  2.54300368e+02 -3.41219081e+02  3.64541448e+02 -1.10002663e+03\n",
      "  4.93685071e+02  5.81690610e+01  5.06832355e+02 -8.10410090e+02\n",
      "  5.44815595e+02  6.71508148e+02 -5.66891015e+02 -1.58811997e+02\n",
      " -5.31772807e+02  9.28590297e+02  7.54809455e+02  7.55943512e+02\n",
      " -4.77313242e+02  5.21414677e+02  2.86961433e+02  2.67952140e+01\n",
      " -1.35517066e+02  1.48579768e+02  1.01842106e+02  5.95265847e+02\n",
      "  4.36429753e+02]\n",
      "Intercept: 137.05180273410895\n",
      "Model formula: 137.052 + 74.333*x0 + -251.583*x1 + 545.837*x2 + 359.551*x3 + -795.721*x4 + 589.909*x5 + -47.121*x6 + 21.686*x7 + 660.900*x8 + 36.973*x9 + 1134.097*x0^2 + 1747.787*x0 x1 + -422.837*x0 x2 + 950.043*x0 x3 + -218.754*x0 x4 + -687.805*x0 x5 + 31.128*x0 x6 + 501.180*x0 x7 + 1059.967*x0 x8 + 682.980*x0 x9 + -1.519*x1^2 + 400.073*x1 x2 + 730.949*x1 x3 + 490.842*x1 x4 + 50.088*x1 x5 + 728.530*x1 x6 + -601.115*x1 x7 + 435.892*x1 x8 + 119.618*x1 x9 + 642.905*x2^2 + 1707.948*x2 x3 + -510.952*x2 x4 + -81.409*x2 x5 + -669.024*x2 x6 + 442.486*x2 x7 + 195.328*x2 x8 + 892.355*x2 x9 + -238.425*x3^2 + 401.348*x3 x4 + 133.228*x3 x5 + 254.300*x3 x6 + -341.219*x3 x7 + 364.541*x3 x8 + -1100.027*x3 x9 + 493.685*x4^2 + 58.169*x4 x5 + 506.832*x4 x6 + -810.410*x4 x7 + 544.816*x4 x8 + 671.508*x4 x9 + -566.891*x5^2 + -158.812*x5 x6 + -531.773*x5 x7 + 928.590*x5 x8 + 754.809*x5 x9 + 755.944*x6^2 + -477.313*x6 x7 + 521.415*x6 x8 + 286.961*x6 x9 + 26.795*x7^2 + -135.517*x7 x8 + 148.580*x7 x9 + 101.842*x8^2 + 595.266*x8 x9 + 436.430*x9^2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# If we use L2 regularization (ridge regression), we end up with a model with smaller coefficients. \n",
    "# In other words, L2 regularization shrinks all the coefficients but rarely turns them into zero.\n",
    "def calculate_mae_with_ridge(X_train, y_train, X_val, y_val, X_test, y_test, degree, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Trains a polynomial regression model with L2 regularization and returns MAE for train, validation, and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    alpha: float, regularization strength (larger values mean more regularization)\n",
    "    \"\"\"\n",
    "    model = Pipeline([\n",
    "        #('scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        ('ridge', Ridge(alpha=alpha))  # Usamos Ridge en lugar de LinearRegression, R\n",
    "    ])\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "    return mae_train, mae_val, mae_test, model\n",
    "\n",
    "\n",
    "# If we use L1 regularization (lasso regression), you end up with a model with fewer coefficients. \n",
    "# In other words, L1 regularization turns some of the coefficients into zero. \n",
    "def calculate_mae_with_lasso(X_train, y_train, X_val, y_val, X_test, y_test, degree, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Trains a polynomial regression model with L1 regularization and returns MAE for train, validation, and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    alpha: float, regularization strength (larger values mean more regularization)\n",
    "    \"\"\"\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Escalamos los datos para mejorar la convergencia de Lasso\n",
    "        ('poly', PolynomialFeatures(degree=degree, include_bias=False)), # Convierte X en [X, X², X³, ...]\n",
    "        ('lasso', Lasso(alpha=alpha, max_iter=10000))  # Aumentamos max_iter para convergencia\n",
    "    ])\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "    return mae_train, mae_val, mae_test, model\n",
    "\n",
    "\n",
    "def find_best_regresion_model(X_train, y_train, X_val, y_val, X_test, y_test, degrees, alphas, calculate_mae_with_regression):\n",
    "    best_degree = None\n",
    "    best_alpha = None\n",
    "    best_train_mae = float('inf')\n",
    "    best_mae_val = float('inf')\n",
    "    best_test_mae = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    for degree in degrees:\n",
    "        for alpha in alphas:\n",
    "            mae_train, mae_val, mae_test, model = calculate_mae_with_regression(X_train, y_train, X_val, y_val, X_test, y_test, degree, alpha)\n",
    "            \n",
    "            # print(f\"Degree: {d}, Alpha: {alpha:.4f}, Train MAE: {mae_train:.2f}, Val MAE: {mae_val:.2f}, Test MAE: {mae_test:.2f}\")\n",
    "            \n",
    "            if mae_val < best_mae_val:\n",
    "                best_model = model\n",
    "                best_train_mae = mae_train\n",
    "                best_mae_val = mae_val\n",
    "                best_test_mae = mae_test\n",
    "                best_degree = degree\n",
    "                best_alpha = alpha\n",
    "                \n",
    "\n",
    "    print(\"Best Model Found:\")\n",
    "    print(f\"Degree: {best_degree}\")\n",
    "    print(f\"Alpha: {best_alpha:.4f}\")\n",
    "    print(f\"Train MAE: {best_train_mae:.2f}\")\n",
    "    print(f\"Validation MAE: {best_mae_val:.2f}\")\n",
    "    print(f\"Test MAE: {best_test_mae:.2f}\")\n",
    "\n",
    "    return best_model, best_degree, best_alpha, best_train_mae, best_mae_val, best_test_mae\n",
    "\n",
    "def get_model_function(model): \n",
    "    poly = model.named_steps['poly']\n",
    "    feature_names = poly.get_feature_names_out(input_features=[f\"x{i}\" for i in range(X_train.shape[1])])\n",
    "    coefs = model.named_steps['ridge'].coef_\n",
    "    intercept = model.named_steps['ridge'].intercept_\n",
    "\n",
    "    terms = [f\"{coef:.3f}*{name}\" for coef, name in zip(coefs, feature_names)]\n",
    "    formula = \" + \".join(terms)\n",
    "    formula = f\"{intercept:.3f} + \" + formula\n",
    "    \n",
    "    return formula\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "\n",
    "# Load and split dataset\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# If we use L2 regularization (ridge regression), we end up with a model with smaller coefficients. \n",
    "degrees = range(1, 11)\n",
    "alphas = [0.00001, 0.001, 0.01, 0.1, 1, 10, 100]  # Diferentes valores de alpha para probar\n",
    "print(\"Finding best Ridge regression model (L2 Norm)...\")\n",
    "best_model, _, _, _, _, _ = find_best_regresion_model(X_train, y_train, X_val, y_val, X_test, y_test, degrees, alphas, calculate_mae_with_ridge)\n",
    "coeficients = best_model.named_steps['ridge'].coef_\n",
    "intercept = best_model.named_steps['ridge'].intercept_\n",
    "print(\"Coefficients:\", coeficients)\n",
    "print(\"Intercept:\", intercept)\n",
    "print(\"Model formula:\", get_model_function(best_model))\n",
    "\n",
    "# If we use L1 regularization (lasso regression), you end up with a model with fewer coefficients. \n",
    "# degrees = range(1, 6)  # Con L1, grados altos pueden ser problemáticos\n",
    "# alphas = [0.0001, 0.001, 0.01, 0.1, 1, 10]  # Alpha necesita valores más pequeños que con L2\n",
    "# print(\"Finding best Lasso regression model (L1 Norm)...\")\n",
    "# find_best_regresion_model(X_train, y_train, X_val, y_val, X_test, y_test,degrees, alphas, calculate_mae_with_lasso)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
