{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b856ac7",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d348bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../../src')\n",
    "import logist_regresion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0738f853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6681877721681662)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Exercise 6.1\n",
    "\n",
    "# A dentist has trained a logistic classifier on a dataset of patients to predict if they have a decayed tooth. The model has determined that the probability that a patient has a decayed tooth is\n",
    "\n",
    "# σ(d + 0.5c – 0.8),\n",
    "\n",
    "# where\n",
    "\n",
    "# d is a variable that indicates whether the patient has had another decayed tooth in the past, and\n",
    "# c is a variable that indicates whether the patient eats candy.\n",
    "# For example, if a patient eats candy, then c = 1, and if they don’t, then c = 0. What is the probability that a patient that eats candy and was treated for a decayed tooth last year has a decayed tooth today?\n",
    "\n",
    "weights = [1, 0.5]\n",
    "bias = - 0.8\n",
    "features = [1, 1]\n",
    "\n",
    "logist_regresion.prediction(weights, bias, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61d07628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction at point p: 0.7310585786300049\n",
      "Log loss at point p: 1.3132616875182228\n",
      "Prediction at point p: 0.6858271738140659\n",
      "Log loss at point p: 1.1578120426234015\n"
     ]
    }
   ],
   "source": [
    "# 2. Exercise 6.2\n",
    "\n",
    "# Consider the logistic classifier that assigns to the point (x1, x2) the prediction ŷ = σ(2x1 + 3x2 – 4), and the point p = (1, 1) with label 0.\n",
    "\n",
    "# Calculate the prediction ŷ that the model gives to the point p.\n",
    "# Calculate the log loss that the model produces at the point p.\n",
    "# Use the logistic trick to obtain a new model that produces a smaller log loss. You can use η = 0.1 as the learning rate.\n",
    "# Find the prediction given by the new model at the point p, and verify that the log loss obtained is smaller than the original.\n",
    "\n",
    "weights = [2, 3]\n",
    "bias = -4\n",
    "features = [1, 1]\n",
    "label = 0\n",
    "\n",
    "predict1 = logist_regresion.prediction(weights, bias, features)\n",
    "print(f\"Prediction at point p: {predict1}\")\n",
    "\n",
    "log_loss1 = logist_regresion.log_loss(weights, bias, features, label)\n",
    "print(f\"Log loss at point p: {log_loss1}\")\n",
    "\n",
    "weights, bias = logist_regresion.logistic_trick(weights, bias, features, label, 0.1)\n",
    "\n",
    "predict2 = logist_regresion.prediction(weights, bias, features)\n",
    "print(f\"Prediction at point p: {predict2}\")\n",
    "\n",
    "log_loss2 = logist_regresion.log_loss(weights, bias, features, label)\n",
    "print(f\"Log loss at point p: {log_loss2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
