{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9bc69b",
   "metadata": {},
   "source": [
    "# Naives Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b4ce4",
   "metadata": {},
   "source": [
    "- The naive Bayes model is a purely probabilistic mode. \n",
    "- The main component of the naive Bayes model is Bayes’ theorem.\n",
    "\n",
    "It’s called naive Bayes because to simplify the calculations, we make a slightly naive assumption that is not necessarily true. However, this assumption helps us come up with a good estimate of the probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67530120",
   "metadata": {},
   "source": [
    "📊 Datos del problema.\n",
    "\n",
    "Precisión (sensibilidad y especificidad):\n",
    "\n",
    "99% de los enfermos dan positivo → sensibilidad = 99%\n",
    "\n",
    "99% de los sanos dan negativo → especificidad = 99%\n",
    "\n",
    "Prevalencia de la enfermedad:\n",
    "\n",
    "Solo 1 de cada 10 000 personas tiene la enfermedad → 0.01%\n",
    "\n",
    "📐 Cálculo Bayesiano (aproximado con 1 millón de personas)\n",
    "\n",
    "En un grupo de 1 000 000 de personas:\n",
    "\n",
    "TP = 1 de cada 10 000 → 100 personas\n",
    "\n",
    "TN = 999 900 personas\n",
    "\n",
    "✔️ Prueba en los enfermos reales:\n",
    "\n",
    "99% de 100 enfermos → 99 dan positivo (true positives)\n",
    "\n",
    "❌ Prueba en los sanos:\n",
    "1% de 999,900 sanos → 9999 dan falsos positivos\n",
    "\n",
    "📦 Total de positivos:\n",
    "\n",
    "Verdaderos positivos: 99\n",
    "\n",
    "Falsos positivos: 9999\n",
    "\n",
    "Total positivos: 99 + 9999 = 10 098\n",
    "\n",
    "\n",
    "p = 99 / 99 + 9999 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e5016e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(+): 0.010098\n"
     ]
    }
   ],
   "source": [
    "p_have_disease = 0.0001\n",
    "p_sick = p_have_disease\n",
    "p_healthy = 1 - p_sick\n",
    "\n",
    "# Supongamos test tiene 99% de sensibilidad y 99% de especificidad\n",
    "p_positive_given_sick = 0.99     # Sensibilidad\n",
    "p_negative_given_healthy = 0.99  # Especificidad\n",
    "p_positive_given_healthy = 1 - p_negative_given_healthy  # Falsos positivos\n",
    "\n",
    "# Probabilidad total de positivo\n",
    "p_total_positive = (p_positive_given_sick * p_sick) + (p_positive_given_healthy * p_healthy)\n",
    "\n",
    "print(f\"P(+): {p_total_positive:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78123c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(+|sick): 0.99\n",
      "P(+): 0.010098\n",
      "P(sick|+): 0.009804\n"
     ]
    }
   ],
   "source": [
    "# Example 3\n",
    "\n",
    "# Data\n",
    "p_sick = 1 / 10000\n",
    "p_healthy = 1 - p_sick\n",
    "p_positivo_given_sick = 0.99    # Sensibilidad del test\n",
    "p_negativo_given_healthy = 0.99  # Especificidad del test\n",
    "p_positivo_given_healthy = 1 - p_negativo_given_healthy  # Falsos positivos\n",
    "\n",
    "# p_positivo_given_sick represente la probabilidad condicional P(+|sick)\n",
    "# p_positivo_given_sick * p_sick no es una probabilidad condicional sino una probabilidad conjunta.\n",
    "\n",
    "\n",
    "# Calcular P(+) usando la ley de probabilidad total\n",
    "p_positivo_total = (p_positivo_given_sick * p_sick) + (p_positivo_given_healthy * p_healthy)\n",
    "\n",
    "# Aplicar teorema de Bayes\n",
    "p_sick_given_positivo = (p_positivo_given_sick * p_sick) / p_positivo_total\n",
    "\n",
    "print(f\"P(+|sick): {p_positivo_given_sick}\")\n",
    "print(f\"P(+): {p_positivo_total:.6f}\")\n",
    "print(f\"P(sick|+): {p_sick_given_positivo:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b468fa2d",
   "metadata": {},
   "source": [
    "Prelude to Bayes’ theorem: The prior, the event, and the posterior\n",
    "\n",
    "**prior** The initial probability\n",
    "\n",
    "**event** Something that occurs, which gives us information\n",
    "\n",
    "**posterior** The final (and more accurate) probability that we calculate using the prior probability and the event\n",
    "\n",
    "An example follows. Imagine that we want to find out the probability that it will rain today. If we don’t know anything, we can come up with only a rough estimate for the probability, which is the prior. If we look around and find out that we are in the Amazon rain forest (the event), then we can come up with a much more exact estimate. In fact, if we are in the Amazon rain forest, it will probably rain today. This new estimate is the posterior.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09640b3e",
   "metadata": {},
   "source": [
    "### Rule of complementary probabilitis\n",
    "\n",
    "P(Ec) = 1 − P(E)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6e9c89",
   "metadata": {},
   "source": [
    "1. First\n",
    "\n",
    "- P(lottery | spam) = 15 = / 20 = 0.75 => the probability that a spam email contains the word lottery.\n",
    "- P(no lottery | spam): = 1 - 0.75 = 0.25 => the probability that a spam email does not contain the word lottery.\n",
    "- P(lottery | ham): 5 / 80 = 0.0625 => the probability that a ham email contains the word lottery.\n",
    "- P(no lottery | ham): 1 - 0.0625 = 0.9375 => the probability that a ham email does not contain\n",
    "\n",
    "![Confusion's Matriz](../../../../images/Emal_spam_ham.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511e4ae5",
   "metadata": {},
   "source": [
    "\n",
    "![Confusion's Matriz](../../../../images/product_rule_probabilities2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ff7785",
   "metadata": {},
   "source": [
    "P(lottery | spam) = 3 / 4 = 0.75\n",
    "\n",
    "P(no lottery | spam) = 1 - P(lottery | spam) = 1 - 0.75 = 0.25\n",
    "\n",
    "P(lottery | ham) = 1 / 16  = 0.0625\n",
    "\n",
    "P(no lottery | ham) = 1 - P(lottery | ham) = 1 - 0.0625 = 0.9375\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9fcbac",
   "metadata": {},
   "source": [
    "## Joint probability\n",
    "\n",
    "The next thing we do is find the probabilities of two events happening at the same time. More specifically, we want the following four probabilities:\n",
    "\n",
    "- The probability that an email is spam and contains the word lottery\n",
    "- The probability that an email is spam and does not contain the word lottery\n",
    "- The probability that an email is ham and contains the word lottery\n",
    "- The probability that an email is ham and does not contain the word lottery\n",
    "These events are called intersections of events and denoted with the symbol ∩. Thus, we need to find the following probabilities:\n",
    "\n",
    "P('lottery' ∩ spam)  = 3 / 4 * 1 / 5 = 3 /20 = 0.15\n",
    " \n",
    "P(no 'lottery' ∩ spam) = 1 / 4 * 1 / 5 = 1 / 20 = 0.05\n",
    "\n",
    "P('lottery' ∩ ham) = 1 / 16 * 4 / 5 =  1 / 20 = 0.05\n",
    "\n",
    "P(no 'lottery' ∩ ham) = 15 / 16 * 4 / 5 = 15 / 20 = 0.75\n",
    "\n",
    "![Confusion's Matriz](../../../../images/product_rule_probabilities.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6cdbdc",
   "metadata": {},
   "source": [
    "## Focus on get P(spam | lottery)\n",
    "\n",
    "- The first one is the probability that an email is spam, and the second one is the probability that the email is ham. These two probabilities don’t add to one. (3/20 + 1/20 = 4/20 = 1/5)\n",
    "- However, because we now live in a world in which the email contains the word lottery, then these two are the only possible scenarios. Thus, their probabilities should add to 1.\n",
    "\n",
    "\n",
    "![Confusion's Matriz](../../../../images/email-spam_lottery.png)\n",
    "\n",
    "To convert to  probabilities thas add to 1:\n",
    "\n",
    "P(lottery ∩ spam) = P(lottery | spam) /  P(lottery | spam) + P(lottery | ham)\n",
    "\n",
    "P(lottery ∩ spam) = 3 / 20 / (3 / 20 + 1 / 20) = 3 / 4\n",
    "\n",
    "P(lottery ∩ ham) = P(lottery | ham) / P(lottery | ham) + P(lottery | spam) \n",
    "\n",
    "P(lottery ∩ ham) = 1 / 20 / (3 / 20 + 1 / 20) = 1 / 4 \n",
    "\n",
    "\n",
    " **Now to calculate the Bayes theorem P(spam | lottery)**\n",
    "\n",
    "P(spam|lottery) = P(lottery ∩ spam) / (P(lottery ∩ spam) + P(lottery ∩ ham))\n",
    "\n",
    "- If we remember what these two probabilities were, using the product rule, we get the following:\n",
    "- Remember that P(lottery | spam) is 3 / 4 (in the case of the  P(+|sick)  this one will be the efficiency of the test (99%), you can see the treemap )\n",
    "\n",
    "P(spam|lottery) = P(lottery | spam) * P(spam) / (P(lottery | spam) * P(spam) + P(lottery | ham) * P(ham))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec453c5",
   "metadata": {},
   "source": [
    "## Now to calculate the Bayes theorem with multiples features\n",
    "\n",
    "- It is based on the premise that when events are independent, the probability of both occurring at the same time is the product of their probabilities\n",
    "\n",
    "\n",
    "P(spam | 'lottery', 'sale', 'mom') = [P('lottery' | spam) * P('sale' | spam) * P('mom' | spam) * P(spam)] /\n",
    "[P('lottery' | spam) * P('sale' | spam) * P('mom' | spam) * P(spam) + P('lottery' | ham) * P('sale' | ham) * P('mom' | ham) * P(ham)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b72b3a",
   "metadata": {},
   "source": [
    "## The naive assumption we’ve made follows:\n",
    "\n",
    "naive assumption The words appearing in an email are completely independent of each other. In other words, the appearance of a particular word in an email in no way affects the appearance of another one.\n",
    "\n",
    "Most likely, the naive assumption is not true. The appearance of one word can sometimes heavily influence the appearance of another. For example, if an email contains the word salt, then the word pepper is more likely to appear in this email, because many times they go together. This is why our assumption is naive. However, it turns out that this assumption works well in practice, and it simplifies our math a lot. It is called the product rule for probabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supervised-machine-learning-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
