{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62d6bcf",
   "metadata": {},
   "source": [
    "# How do you measure classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8a36e",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    " The simplest and most common measure of classification models. The accuracy of a model is the percentage of times that a model is correct.  In other words, it is the ratio between the number of correctly predicted data points and the total number of data points.\n",
    " For example, if we evaluate a model on a test dataset of 1,000 samples, and the model predicted the correct label of the samples 875 times, then this model has an accuracy of 875/1000 = 0.875, or 87.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca16ca",
   "metadata": {},
   "source": [
    "## How to fix the accuracy problem? Defining different types of errors and how to measure them\n",
    "\n",
    "The first thing we need to study is types of errors. In the next section, we see that some errors are more critical than others. Then in the sections “Storing the correctly and incorrectly\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b729c3",
   "metadata": {},
   "source": [
    "### False positives and false negatives: Which one is worse?\n",
    "\n",
    "In many cases, the total number of errors doesn’t tell us everything about the model’s performance, and we need to dig in deeper and identify certain types of errors in different ways. In this section, we see two types of errors. What are the two types of errors that the coronavirus model can make? It can diagnose a healthy person as sick or a sick person as healthy. In our model, we label the sick patients as positive, by convention. The two error types are called false positives and negatives, as follows:\n",
    "\n",
    "- False positive: a healthy person who is incorrectly diagnosed as sick\n",
    "- False negative: a sick person who is incorrectly diagnosed as healthy\n",
    "\n",
    "In the general setting, a false positive is a data point that has a negative label, but the model falsely classifies it as positive. A false negative is a data point that has a positive label, but the model falsely classified it as negative. Naturally, the cases that are correctly diagnosed also have names, as follows:\n",
    "\n",
    "- True positive: a sick person who is diagnosed as sick\n",
    "- True negative: a healthy person who is diagnosed as healthy\n",
    "\n",
    "Now, let’s look at the email dataset. Let’s say we have a model that predicts whether each email is spam or ham. We consider the positives to be the spam emails. Therefore, our two types of errors follow:\n",
    "\n",
    "- False positive: a ham email that is incorrectly classified as spam\n",
    "- False negative: a spam email that is incorrectly classified as ham\n",
    "\n",
    "And the correctly classified emails are the following:\n",
    "\n",
    "- True positive: a spam email that is correctly classified as spam\n",
    "- True negative: a ham email that is correctly classified as ham\n",
    "\n",
    "\n",
    " **Let’s say that when we diagnose a patient as healthy, we send them home with no treatment, and that when we diagnose a patient as sick, we send them for more tests. n the coronavirus model, a false negative is much worse than a false positive.**\n",
    "\n",
    " **Now we’ll do the same analysis on the spam model. In this case, let’s say that if our spam classifier classifies an email as spam, then this email is automatically deleted. in the spam email model, a false positive is much worse than a false negative.**\n",
    "\n",
    " This is where the two models differ. In the coronavirus model, a false negative is worse, whereas in the spam email model, a false positive is worse. The problem with measuring the accuracy in any of these two models is that the accuracy considers both types of errors as equally serious and doesn’t tell them apart.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f563f557",
   "metadata": {},
   "source": [
    "### The confusion matrix\n",
    "\n",
    "\n",
    "Person’s condition | Predicted positive       | Predicted negative       |\n",
    "|--------------------|--------------------------|--------------------------|\n",
    "| Positive           | Number of true positives | Number of false negatives |\n",
    "| Negative           | Number of false positives | Number of true negatives |\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------------------------------------\n",
    "\n",
    "| Coronavirus model 1 | Diagnosed sick (predicted positive) | Diagnosed healthy (predicted negative) |\n",
    "|---------------------|-------------------------------------|----------------------------------------|\n",
    "| Sick (positive)     | 0 (number of true positives)        | 10 (number of false negatives)         |\n",
    "| Healthy (negative)  | 0 (number of false positives)       | 990 (number of true negatives)         |\n",
    "\n",
    "------------------------------------------------------------------------------------------------------\n",
    "| Coronavirus model 2 | Diagnosed sick (predicted positive) | Diagnosed healthy (predicted negative) |\n",
    "|---------------------|-------------------------------------|----------------------------------------|\n",
    "| Sick (positive)     | 8 (number of true positives)        | 2 (number of false negatives)         |\n",
    "| Healthy (negative)  | 48 (number of false positives)       | 942 (number of true negatives)         |\n",
    "\n",
    "------------------------------------------------------------------------------------------------------\n",
    "| Spam model 1 | Predicted spam          | Predicted ham           |\n",
    "|--------------|--------------------------|--------------------------|\n",
    "| Spam         | 30 (true positives)      | 10 (false negatives)     |\n",
    "| Ham          | 5 (false positives)      | 55 (true negatives)      |\n",
    "------------------------------------------------------------------------------------------------------\n",
    "| Spam model 2 | Predicted spam          | Predicted ham           |\n",
    "|--------------|--------------------------|--------------------------|\n",
    "| Spam         | 35 (true positives)      | 5 (false negatives)      |\n",
    "| Ham          | 10 (false positives)     | 50 (true negatives)      |\n",
    "------------------------------------------------------------------------------------------------------\n",
    "| Actual \\ Predicted | Dog                      | Cat                      | Bird                     |\n",
    "|--------------------|--------------------------|--------------------------|--------------------------|\n",
    "| **Dog**            | Number of true positives | Number of false negatives (Dog→Cat) | Number of false negatives (Dog→Bird) |\n",
    "| **Cat**            | Number of false negatives (Cat→Dog) | Number of true positives | Number of false negatives (Cat→Bird) |\n",
    "| **Bird**           | Number of false negatives (Bird→Dog) | Number of false negatives (Bird→Cat) | Number of true positives |\n",
    "------------------------------------------------------------------------------------------------------\n",
    "\n",
    "For problems with more classes, we have a larger confusion matrix. For example, if our model classifies images into aardvarks, birds, cats, and dogs, then our confusion matrix is a four-by-four matrix, where along the rows we have the true labels (the type of animal), and along the columns we have the predicted labels (the type of animal that the model predicted). This confusion matrix also has the property that the correctly classified points are counted in the diagonal, and the incorrectly classified are counted off the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1021df",
   "metadata": {},
   "source": [
    "## Recall: Among the positive examples, how many did we correctly classify?\n",
    "\n",
    "This is the key. This new metric, called recall, measures precisely that: out of the sick people, how many did our model diagnose correctly?\n",
    "\n",
    "Recall finds the proportion of correct predictions among the data points with a positive label.\n",
    "\n",
    "Recall = True positives / True positives + False negatives\n",
    "\n",
    "because the False negatives should be Positives\n",
    "\n",
    "Coronavirus Model 1:\n",
    "\n",
    "True positives (sick patients diagnosed sick and sent for more tests) = 0\n",
    "\n",
    "False negatives (sick patients diagnosed healthy and sent home) = 10\n",
    "\n",
    "Recall = 0/10 = 0%\n",
    "\n",
    "Coronavirus Model 2:\n",
    "\n",
    "True positives (sick patients diagnosed sick and sent for more tests) = 8\n",
    "\n",
    "False negatives (sick patients diagnosed healthy and sent home) = 2\n",
    "\n",
    "Recall = 8/10 = 80%\n",
    "\n",
    "Models like the coronavirus model, in which false negatives are much more expensive than false positives, are high recall models.\n",
    "\n",
    "**could we fool this metric in the same way we fooled accuracy? In other words, can we build a model that has total recall? Well, get ready for a surprise, because we can. If we build a model that diagnoses every patient as sick, this model has a 100% recall. However, this model is terrible, too, because although it has zero false negatives, it has too many false positives to make it a good model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e5620",
   "metadata": {},
   "source": [
    "## Precision: Among the examples we classified as positive, how many did we correctly classify?\n",
    "\n",
    "we learn about a similar metric, precision, which measures how well our model does with false positives. We’ll use this metric to evaluate the spam email model, because this model can’t afford to have too many false positives.\n",
    "\n",
    "Precision = True positives / True positves + false positives\n",
    "\n",
    "Spam Model 1:\n",
    "\n",
    "True positives (spam emails deleted) = 30\n",
    "False positives (ham emails deleted) = 5\n",
    "Precision = 30/35 = 85.7%\n",
    "Spam Model 2:\n",
    "\n",
    "True positives (spam emails deleted) = 35\n",
    "False positives (ham emails deleted) = 10\n",
    "Precision = 35/45 = 77.7%\n",
    "\n",
    "Just as we thought: precision gave a higher score to the first model than to the second model. We conclude that models like the spam model, in which false positives are much more expensive than false negatives, are high precision models\n",
    "\n",
    "** we can also trick precision. Consider the following spam filter: a spam filter that never detects any spam. What is the precision of this model? This is complicated, because there are zero spam emails deleted (zero true positives) and zero ham emails deleted (zero false positives). We won’t attempt to divide zero over zero, because this book would burst into flames, but by convention, a model that makes no false positive mistakes has a precision of 100%. But, of course, a spam filter that does nothing is not a good spam filter.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e029de57",
   "metadata": {},
   "source": [
    "This goes to show that no matter how good our metrics are, they can always be fooled. That doesn’t mean they don’t work. Accuracy, precision, and recall are useful tools in a data scientist’s toolbox. It is up to us to decide which ones are good for our model, by deciding what errors are more expensive than others. Always be careful to not fall into the trap of thinking that a model is good before evaluating it with different metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a73787",
   "metadata": {},
   "source": [
    "## Combining recall and precision as a way to optimize both: The F-score\n",
    "\n",
    "we discuss the F-score, a metric that combines both recall and precision\n",
    "\n",
    "A good model is one that has good recall and good precision.\n",
    "\n",
    " in real life, both are important, even if they are important to different degrees. For example, we may want a model that doesn’t misdiagnose any sick person but that also doesn’t misdiagnose too many healthy people, because misdiagnosing a healthy person may involve unnecessary and painful testing, or even an unnecessary surgery, which could affect their health negatively. In the same way, we may want a model that doesn’t delete any of our good emails. But to be a good spam filter, it still needs to catch a lot of spam; otherwise, it’s useless. The F-score has a parameter β accompanying it, so the more common term is Fβ-score. When β = 1, it is called the F1-score.\n",
    "\n",
    "\n",
    " A quantity that is like the average of two numbers is called the harmonic mean. Whereas the average of two numbers a and b is (a + b)/2, their harmonic mean is 2ab/(a + b).\n",
    "\n",
    " F1-score = 2ab / (a + b).\n",
    "\n",
    "\n",
    " F1 = 2 * precision * recall / precision + recall\n",
    "\n",
    " If both numbers are high, the F1-score is high. However, if one of them is low, the F1-score will be low. The purpose of the F1-score is to measure if both recall and precision are high and to ring a bell when one of these two scores is low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ddd64",
   "metadata": {},
   "source": [
    "## Calculating the Fβ-score\n",
    "\n",
    "In the previous subsection, we learned about the F1-score, a score that combines recall and precision, for the purpose of evaluating a model. However, sometimes we want more recall than precision, or vice versa\n",
    "\n",
    "This means that sometimes we may want a model that cares both about false positives and false negatives but assigns more weight to one of them. For example, the coronavirus model cares much more about false negatives, because people’s lives may depend on a correct identification of the virus, but it still doesn’t want to create too many false positives, because we may not want to spend excessive resources retesting healthy people\n",
    "\n",
    "The point of β is to act as a dial that we turn to emphasize precision or recall. More specifically, if we slide the β dial to zero, we get full precision; if we slide it to infinity, we get full recall. In general, the lower the value of β, the more we emphasize precision, and the higher the value of β, the more we emphasize recall.\n",
    "\n",
    "Fβ = ((1 + β²) * P * R) / (β² * P + R)\n",
    "\n",
    "- Si β = 1, se obtiene el clásico F<sub>1</sub>-score, que balancea precisión y recall por igual.\n",
    "\n",
    "- Si β > 1, se da más peso al recall.\n",
    "\n",
    "- Si β < 1, se da más peso a la precisión.\n",
    "\n",
    "- β² → ∞ ⇒ F<sub>β</sub> ≈ Recall\n",
    "- β² → 0 ⇒ F<sub>β</sub> ≈ Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d388c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(num_predict_right, n):\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "    return num_predict_right / n\n",
    "\n",
    "def calc_recall(true_positive, false_negative):\n",
    "    if true_positive + false_negative == 0:\n",
    "        return 0.0\n",
    "    return true_positive / (true_positive + false_negative)\n",
    "\n",
    "def calc_precision(true_positive, false_positive):\n",
    "    if true_positive + false_positive == 0:\n",
    "        return 0.0\n",
    "    return true_positive / (true_positive + false_positive)\n",
    "\n",
    "def calc_f1_score(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def calc_f_beta_score(precision, recall, beta):\n",
    "    if precision + beta**2 * recall == 0:\n",
    "        return 0.0\n",
    "    return (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb0dc8",
   "metadata": {},
   "source": [
    "## The receiver operating characteristic (ROC) curve\n",
    "\n",
    "A way to optimize sensitivity and specificity in a model\n",
    "\n",
    "Sensitivity (true positive rate): the capacity of the test to identify the positively labeled points. This is the ratio between the number of true positives and the total number of positives. (Note: this is the same as recall). it's the same that recall; “Recall is sensitivity, but precision and specificity are different.”\n",
    "\n",
    "**Sensitivity (true positive rate) = true positive / true positive + false negative**\n",
    "\n",
    "Specificity (true negative rate): the capacity of the test to identify the negatively labeled points. This is the ratio between the number of true negatives and the total number of negatives.\n",
    "\n",
    "**Specificity (true negative rate) = true negative / true negative + false positive**\n",
    "\n",
    "**The first and only assumption we need to make about our model is that it returns the prediction as a continuous value, namely, as a probability.--\n",
    "\n",
    "- lower threadhold implying  high sensitivity (few false negativesand and many negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42864027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sensitive(true_positive, false_negative):\n",
    "    if true_positive + false_negative == 0:\n",
    "        return 0.0\n",
    "    return true_positive / (true_positive + false_negative)\n",
    "\n",
    "def calc_specificity(true_negative, false_positive):\n",
    "    if true_negative + false_positive == 0:\n",
    "        return 0.0\n",
    "    return true_negative / (true_negative + false_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fca0417",
   "metadata": {},
   "source": [
    "\n",
    "For example, when we think of recall, think of a car company that made a car with a fatal design flaw. They need to find all the faulty cars and recall them. If they accidentally get more cars that are not faulty, they simply return them. However, not finding one of the faulty cars would be terrible. Thus, recall cares about finding all the positively labeled examples. This represents a model with high recall.\n",
    "\n",
    "On the other hand, if we work for this car company, and we went a little overboard and started recalling all the cars, our boss may come over and say, “Hey, you are sending too many cars to fix, and we are running out of resources. Can you please be more selective and send me precisely those that are faulty?” Then we need to add precision to the model and try to find only the ones that are faulty, even if we accidentally miss some of the faulty ones (hopefully not!). This represents a model with high precision.\n",
    "\n",
    "When it comes to specificity and sensitivity, think of an earthquake sensor that beeps every time there is an earthquake. This sensor is tremendously sensitive. If a butterfly sneezes in the next house, the sensor beeps. This sensor will capture all the earthquakes for sure, but it will also capture many other things that are not an earthquake. This represents a model with high sensitivity.\n",
    "\n",
    "![Confusion's Matriz](../../../images/Confusion%20Matriz.PNG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
